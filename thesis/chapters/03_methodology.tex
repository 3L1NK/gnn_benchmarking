This chapter specifies the data contract, modeling contract, and evaluation contract used by the benchmark protocol. The design goal is that every number in Chapter~5 can be traced to deterministic code paths and explicit hyperparameters.

\section{Data Contract}

\subsection{Universe Definition}
The benchmark universe is defined in \texttt{data/processed/universe.csv}. The current canonical universe contains 32 liquid US tickers across 13 sectors and 23 industries. It includes both equities and macro-sensitive ETFs (for example, fixed-income and commodity exposure) to make cross-asset relation modeling meaningful under one common protocol.

The universe is fixed before model training. The same universe file is consumed by all families (XGBoost, LSTM, graph-feature, and GNN variants), so cross-family comparison does not depend on changing asset sets.

\subsection{Time Span and Trading Calendar}
The processed panel spans March~29,~2000 to December~30,~2024 with 6,228 distinct trading days after preprocessing. The supervised benchmark split is:
\begin{itemize}
  \item train: dates before January~1,~2016,
  \item validation: January~1,~2016 to December~31,~2019,
  \item test: January~2,~2020 to December~27,~2024.
\end{itemize}

This is strictly chronological and enforced by protocol guards, preventing leakage from shuffled or random folds.

\subsection{Raw Data Source and Price Definition}
Raw data are downloaded from Yahoo Finance through \texttt{yfinance} with \texttt{auto\_adjust=False}, then normalized into a tidy panel by the preprocessing scripts. The processed training panel uses closing price and volume columns; feature engineering then derives return, momentum, volatility, drawdown, and oscillator features.

In other words, the benchmark uses a reproducible point-in-time transformation pipeline from raw close/volume observations rather than manual spreadsheet curation.

\subsection{Missing Data and Coverage Handling}
The processed feature table drops rows that are invalid after rolling-window feature construction. The baseline portfolio utilities further enforce fixed-universe consistency by:
\begin{itemize}
  \item removing tickers with no valid start price,
  \item forward-filling short gaps with a capped gap length,
  \item removing residual missing series,
  \item aligning calendars by intersection of valid dates.
\end{itemize}
This prevents hidden survivorship-like drift in baseline comparisons.

\subsection{Feature Set (Exact Columns)}
The canonical engineered features include:
\begin{itemize}
  \item returns: \texttt{ret\_1d}, \texttt{ret\_5d}, \texttt{ret\_20d}, \texttt{log\_ret\_1d},
  \item momentum: \texttt{mom\_3d}, \texttt{mom\_10}, \texttt{mom\_21d},
  \item realized volatility: \texttt{vol\_5d}, \texttt{vol\_20d}, \texttt{vol\_60d},
  \item path risk: \texttt{drawdown\_20d},
  \item volume signals: \texttt{volume\_pct\_change}, \texttt{vol\_z\_5}, \texttt{vol\_z\_20},
  \item technical oscillators: \texttt{rsi\_14}, \texttt{macd\_line}, \texttt{macd\_signal}, \texttt{macd\_hist}.
\end{itemize}

\subsection{Target Definition}
The supervised target is next-day log return:
\[
y_{i,t+1} = \log\left(\frac{P_{i,t+1}}{P_{i,t}}\right),
\]
implemented as a one-step forward shift within each ticker series. Log returns are used because they compose additively over time and remain numerically stable for small daily moves.

\subsection{Leakage Controls}
Leakage controls are built into both data and graph pipelines:
\begin{itemize}
  \item split masks are chronological and shared across families,
  \item feature scaling is fit on train only,
  \item graph windows are constrained to history up to each snapshot date,
  \item Granger edge discovery is fit on train-window data and reused on later splits,
  \item protocol hash fields are recorded in the ledger for auditability.
\end{itemize}

\section{Formal Learning Setup}
Let $\mathcal{U}_t$ denote the tradable universe at date $t$, with node features $\mathbf{x}_{i,t}$ and optional graph $G_t$. The predictor is
\[
\hat{y}_{i,t+1} = f_{\theta}(\mathbf{x}_{i,t}, G_t),
\]
where $G_t$ is omitted for non-graph baselines. Forecasts are converted into cross-sectional ranks and fed into a long-only portfolio layer.

\section{Model Families}
The matrix includes four categories:
\begin{itemize}
  \item non-graph baselines: \texttt{xgb\_raw}, \texttt{lstm},
  \item graph-feature baseline: \texttt{xgb\_node2vec\_corr},
  \item static GNN ablations: GCN/GAT with \texttt{corr}, \texttt{sector}, \texttt{granger}, and \texttt{corr+sector+granger},
  \item temporal-labeled static variants: \texttt{tgcn\_static\_*}, \texttt{tgat\_static\_*}.
\end{itemize}

This setup tests both end-to-end message passing and ``graph as feature engineering'' strategies under one evaluation shell.

\section{Graph Construction (Explicit Parameters)}

\subsection{Correlation Edges}
Correlation edges are built from rolling return windows with:
\begin{itemize}
  \item \texttt{corr\_window=60} trading days,
  \item absolute threshold \texttt{corr\_threshold=0.4},
  \item per-node degree control via \texttt{corr\_top\_k}.
\end{itemize}
Weights are normalized and scaled by per-channel coefficients before merge.

\subsection{Sector and Industry Edges}
Sector/industry relations come from \texttt{universe.csv}. Sector and industry matches are encoded as typed priors and merged with correlation and Granger maps under configured scalar weights.

\subsection{Granger Edges}
Granger relations use pairwise tests on train-window returns only. Canonical core/tuned configs map to:
\begin{itemize}
  \item max lag: 5,
  \item p-threshold: 0.01,
  \item outgoing edge budget: top-50.
\end{itemize}
Edges are not re-estimated on validation/test slices, preserving causal chronology.

\subsection{Graphical Lasso Variants}
Graphical Lasso variants are included as exploratory/graph-feature references with explicit regularization parameter $\alpha$ and train-window fitting only. They are not used to define the main 13-model tuned matrix.

\section{Training and Hyperparameter Search}
Training follows one CLI entrypoint and shared protocol enforcement. The tuned-all matrix uses a random-search tuning objective of validation backtest annualized Sharpe. Budget profile \texttt{medium} corresponds to:
\begin{itemize}
  \item GNN trials: 12,
  \item XGBoost trials: 24,
  \item LSTM trials: 18.
\end{itemize}

Early stopping and patience are configured per family, and seed defaults are fixed for deterministic reruns.

\section{Portfolio Layer}
Forecasts are monetized using a long-only top-$k$ strategy with equal weights among selected assets.
\[
w_{i,t}=
\begin{cases}
1/k, & i\in\pi_t,\\
0, & \text{otherwise.}
\end{cases}
\]
Canonical settings:
\begin{itemize}
  \item \texttt{top\_k=20},
  \item transaction cost: 5 bps,
  \item rebalance policies: 1 and 5 trading days.
\end{itemize}
Turnover is measured as the L1 change in weights,
\[
\text{turnover}_t = \sum_i |w_{i,t}-w_{i,t^-}|,
\]
and transaction cost is applied as turnover multiplied by cost-per-unit notional.

\section{Evaluation Metrics}
\subsection{Prediction Metrics}
\begin{itemize}
  \item RMSE,
  \item MAE,
  \item mean Rank IC.
\end{itemize}

\subsection{Portfolio Metrics}
\begin{itemize}
  \item annualized return,
  \item annualized volatility,
  \item daily and annualized Sharpe ratio \citep{sharpe1994sharpe},
  \item annualized Sortino,
  \item maximum drawdown,
  \item average turnover,
  \item final portfolio value.
\end{itemize}

\subsection{Fail-Fast Metric Integrity}
The reporting stack enforces:
\begin{itemize}
  \item no empty return series,
  \item no near-constant return series for Sharpe estimation,
  \item finite metric values in thesis-critical columns,
  \item run-key uniqueness before aggregation.
\end{itemize}
If these checks fail, report generation stops with explicit diagnostics instead of silently dropping runs.

\section{Decision Rule for Deployment Ranking}
The decision ranking is lexicographic:
\begin{enumerate}
  \item annualized Sharpe (higher),
  \item max drawdown (less negative),
  \item Rank IC (higher),
  \item turnover (lower).
\end{enumerate}
This reflects a deployment preference for risk-adjusted quality first, with turnover-aware tie breaking.

\section{Methodological Scope}
The thesis is intentionally a controlled benchmark, not an architecture zoo. The contribution is decision-quality evidence under one auditable protocol rather than a claim that any single model class is universally superior \citep{fama1970efficient,markowitz1952portfolio,kipf2017semi,velickovic2018gat,chen2016xgboost,hochreiter1997lstm,grover2016node2vec}.
